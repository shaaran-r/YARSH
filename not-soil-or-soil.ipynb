{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Soil or Not-Soil?","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Required import statements.\n\nimport os, random, numpy as np, pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch, torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset, WeightedRandomSampler\nfrom torchvision import datasets, transforms\nfrom timm import create_model\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Random seed, for maintaining reproducability and consistency\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed()\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Paths\nSOIL_IMG_DIR = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train\"\nSOIL_LABEL_CSV = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv\"\n\n# Hyperparameters\nIMG_SIZE = 224\nBATCH_SIZE = 32\nEPOCHS = 10\nLR = 3e-4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image preprocessing\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.4, 0.4, 0.4),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\nval_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Dataset\nclass SoilDataset(Dataset):\n    def __init__(self, df, img_dir, transform, label=1.0):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.label = label\n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['image_id'])\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n        return image, torch.tensor(self.label, dtype=torch.float32)\n\n#CIFAR is used for negative sample preprocessing, i.e images without soil data, to avoid data bias.\nclass CIFARWrapper(Dataset):\n    def __init__(self, dataset, transform, label=0.0):\n        self.dataset = dataset\n        self.transform = transform\n        self.label = label\n\n    def __len__(self): return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, _ = self.dataset[idx]\n        image = self.transform(image)\n        return image, torch.tensor(self.label, dtype=torch.float32)\n\n# Load and split\nsoil_df = pd.read_csv(SOIL_LABEL_CSV)\ntrain_df, val_df = train_test_split(soil_df, test_size=0.2, random_state=42) #test size 0.2 says that there is an 80:20 (training:validation) split\n\n# Soil datasets (positive examples, i.e images containing soil)\nsoil_train = SoilDataset(train_df, SOIL_IMG_DIR, train_transform, 1.0)\nsoil_val = SoilDataset(val_df, SOIL_IMG_DIR, val_transform, 1.0)\n\n# CIFAR10 negatives\ncifar10_train = datasets.CIFAR10(\".\", train=True, download=True)\ncifar10_val = datasets.CIFAR10(\".\", train=False, download=True)\n\n# Wrap harder negatives. hard classes are those which the model finds difficult to classify from the positive class. \nhard_classes = [2, 3, 4, 5, 6]  # bird, cat, deer, dog, frog\nhard_train = [(x, y) for x, y in zip(cifar10_train.data, cifar10_train.targets) if y in hard_classes]\nhard_val = [(x, y) for x, y in zip(cifar10_val.data, cifar10_val.targets) if y in hard_classes]\n\n# NumpyImageDataset allows to use image data stored in NumPy arrays with PyTorch's data loading utilities, and preprocessing.\nclass NumpyImageDataset(Dataset):\n    def __init__(self, data, transform, label=0.0):\n        self.data = data\n        self.transform = transform\n        self.label = label\n\n    def __len__(self): return len(self.data)\n\n    def __getitem__(self, idx):\n        image = Image.fromarray(self.data[idx][0] if isinstance(self.data[idx], tuple) else self.data[idx])\n        image = self.transform(image)\n        return image, torch.tensor(self.label, dtype=torch.float32)\n\nnon_soil_train = NumpyImageDataset(hard_train[:len(train_df)], train_transform)\nnon_soil_val = NumpyImageDataset(hard_val[:len(val_df)], val_transform)\n\n# Combine datasets\ntrain_dataset = ConcatDataset([soil_train, non_soil_train])\nval_dataset = ConcatDataset([soil_val, non_soil_val])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Focal Loss is designed to address class imbalance by down-weighting easy examples and focusing training on hard, misclassified examples.\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n    def forward(self, inputs, targets):\n        bce_loss = self.bce(inputs, targets)\n        pt = torch.exp(-bce_loss)\n        focal = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal.mean()\n\n# Model: ConvNeXt, which is a CNN model by meta developed in 2022. It is comparable to ViT and improved version of ResNet.\nmodel = create_model(\"convnext_base\", pretrained=True, num_classes=1).to(DEVICE)\ncriterion = FocalLoss()\noptimizer = optim.AdamW(model.parameters(), lr=LR)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# Mixed Precision, for faster training and less load on GPU.\nscaler = torch.cuda.amp.GradScaler()\n\n# Training the model\ndef train():\n    for epoch in range(EPOCHS):\n        model.train()\n        total_loss = 0\n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n            images, labels = images.to(DEVICE), labels.to(DEVICE).unsqueeze(1)\n            optimizer.zero_grad() #zero-grad optimizer\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n        scheduler.step()\n        print(f\"Epoch {epoch+1}: Train Loss = {total_loss/len(train_loader):.4f}\")\n        validate()\n\ndef validate():\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE).unsqueeze(1)\n            outputs = torch.sigmoid(model(images))\n            preds = (outputs > 0.5).float()\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n\ntrain()\ntorch.save(model.state_dict(), \"convnext_soil_classifier.pth\") #saving the pytorch model\nprint(\"Model saved to convnext_soil_classifier.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T18:08:01.672421Z","iopub.execute_input":"2025-05-22T18:08:01.673045Z","iopub.status.idle":"2025-05-22T18:12:52.767618Z","shell.execute_reply.started":"2025-05-22T18:08:01.673014Z","shell.execute_reply":"2025-05-22T18:12:52.766746Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd4c3618c2946378a0606cf8c4bf0ae"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/981703507.py:136: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\nEpoch 1:   0%|          | 0/62 [00:00<?, ?it/s]/tmp/ipykernel_35/981703507.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1: 100%|██████████| 62/62 [00:22<00:00,  2.80it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 0.0057\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 99.80%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 62/62 [00:22<00:00,  2.71it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 0.0017\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 62/62 [00:22<00:00,  2.72it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 0.0007\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 62/62 [00:22<00:00,  2.81it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss = 0.0011\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 99.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 62/62 [00:22<00:00,  2.76it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss = 0.0007\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 62/62 [00:22<00:00,  2.82it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss = 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 62/62 [00:21<00:00,  2.82it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss = 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 62/62 [00:22<00:00,  2.81it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss = 0.0010\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 98.98%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 62/62 [00:22<00:00,  2.77it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss = 0.0004\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 62/62 [00:22<00:00,  2.77it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss = 0.0001\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 100.00%\nModel saved to convnext_soil_classifier.pth\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Testing on \"test\" images and storing the predictions in a .csv file","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torchvision import transforms\nfrom timm import create_model\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing test images and setting the hyperparameters.\nTEST_IMG_DIR = \"/kaggle/input/soil-classification-part-2/soil_competition-2025/test\"\nMODEL_PATH = \"/kaggle/working/convnext_soil_classifier.pth\"\nBATCH_SIZE = 32\nIMG_SIZE = 224\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Transforms (same as validation)\ntest_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\n# Dataset for test\nclass TestDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.image_ids = sorted(os.listdir(img_dir))\n\n    def __len__(self): return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        img_name = self.image_ids[idx]\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n        return image, img_name","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_dataset = TestDataset(TEST_IMG_DIR, test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Load model\nmodel = create_model(\"convnext_base\", pretrained=False, num_classes=1)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.to(DEVICE)\nmodel.eval()\n\n# Predict\nresults = []\nwith torch.no_grad():\n    for images, filenames in tqdm(test_loader, desc=\"Predicting\"):\n        images = images.to(DEVICE)\n        outputs = torch.sigmoid(model(images)).squeeze(1)\n        preds = (outputs > 0.5).long().cpu().numpy()\n        results.extend(zip(filenames, preds))\n\n# Creating submission output as CSV file. \nsubmission_df = pd.DataFrame(results, columns=[\"image_id\", \"label\"])\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"Saved predictions to submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T18:15:40.910329Z","iopub.execute_input":"2025-05-22T18:15:40.910603Z","iopub.status.idle":"2025-05-22T18:15:56.077325Z","shell.execute_reply.started":"2025-05-22T18:15:40.910584Z","shell.execute_reply":"2025-05-22T18:15:56.076528Z"}},"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 31/31 [00:13<00:00,  2.36it/s]","output_type":"stream"},{"name":"stdout","text":"Saved predictions to submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8}]}